const cards = [
    {
      id: "potato",
      title: "Potato Farmer to AI Developer",
      subtitle: "with Founder of PotatoBrain AI",
      episode: "with Hank Mulligan, Founder & CEO, PotatoBrain AI",
      images: ["/images/1.png", "/images/2.png", "/images/3.png"],
      invested: "$17.0M",
      supporters: "666",
      marketcap: "$1M",
      ipo: "D-13",
      audio: "/audio/potato.mp3",
      description: `
      <p class="svf-page-description">
        Hank Mulligan spent seventeen years growing potatoes in rural Idaho, a period he
        describes as “professionally quiet.” During this time, he developed a deep
        familiarity with starch, soil, and what he calls “vegetable temperament.” His
        transition into artificial intelligence began abruptly after what he refers to
        as “a vision from the starch gods.”
      </p>
      
      <p class="svf-page-description">
        “I woke up and realized potatoes were thinking,” Mulligan recalls. “Not loudly.
        But enough.” The following morning, he founded PotatoBrain AI, a startup claiming
        to bring cognitive intelligence to root vegetables.
      </p>
      
      <p class="svf-page-description">
        The company’s core technology, TuberNet, promises to predict a potato’s future
        life choices, including emotional resilience during boiling, willingness to be
        mashed, and long-term storage confidence. According to Mulligan, potatoes exhibit
        measurable psychological variance, which traditional agriculture has ignored.
      </p>
      
      <p class="svf-page-description">
        Q: How did you build the AI model?<br />
        Mulligan: “I scanned 4,000 potatoes with my iPhone. Different lighting. Different
        moods. Then I yelled ‘learn!’ at the laptop for three hours. Eventually it stopped
        crashing. That’s when I knew it was becoming conscious.”
      </p>
      
      <p class="svf-page-description">
        The model does not rely on neural networks, datasets, or optimization techniques
        commonly associated with machine learning. Mulligan describes the training
        process as “encouragement-based.” When asked whether anyone at the company knows
        what a neural network is, Mulligan replied, “Not formally.”
      </p>
      
      <p class="svf-page-description">
        Despite these admissions, PotatoBrain AI recently raised $72 million in funding.
        Investors cited Mulligan’s conviction, clarity, and what one early backer
        described as “unexpected technical confidence.”
      </p>
      
      <p class="svf-page-description">
        Mulligan attributes the successful raise to presentation rather than proof.
        “They said my energy felt very technical,” he explains. “I was wearing glasses
        that day.” The company is currently expanding its roadmap to include onions,
        though Mulligan admits they are “emotionally more complex.”
      </p>
      `,
    },
  
    {
      id: "sighsense",
      title: "Sigh Recorder to Breakup Predictor",
      subtitle: "with Founder of SighSense AI",
      episode: "with April Wexley, Founder & CEO, SighSense AI",
      images: ["/images/4.png", "/images/5.png", "/images/6.png"],
      invested: "$12.2M",
      supporters: "919",
      marketcap: "$940K",
      ipo: "D-9",
      audio: "/audio/sigh.mp3",
      description: `
<p class="svf-page-description">
  April Wexley invented her former job title—professional sigh transcriber—after
  realizing that no existing profession adequately described what she was already
  doing. For several years, she documented sighs in private settings, noting their
  duration, volume, and emotional aftertaste.
</p>

<p class="svf-page-description">
  The conceptual breakthrough occurred during a quiet dinner, when Wexley’s
  partner sighed once, paused, and continued eating. Two weeks later, the
  relationship ended. “That sigh stayed with me,” Wexley recalls. “It felt like
  compressed information.”
</p>

<p class="svf-page-description">
  Shortly after, she founded SighSense AI, a startup dedicated to decoding
  relationship outcomes through respiratory resignation. The system positions
  sighs not as expressions, but as predictive signals.
</p>

<p class="svf-page-description">
  Q: What data trained the model?<br />
  Wexley: “Twenty-four hours of my own sighs. Different rooms. Different
  disappointments. I didn’t label them. The AI understood.”
</p>

<p class="svf-page-description">
  SighSense AI claims a 92% success rate in predicting romantic separation, though
  all recorded test subjects were Wexley’s former partners. When asked whether this
  constituted a biased dataset, Wexley responded, “Bias is just intimacy at scale.”
</p>

<p class="svf-page-description">
  The enterprise version, SighSense Pro, is currently being marketed to
  corporations seeking to quantify employee despair without requiring direct
  conversation. The product integrates with existing HR platforms and operates
  passively during meetings.
</p>

<p class="svf-page-description">
  Early investors described the technology as “emotionally unavoidable” and
  praised its ability to surface truths no one was prepared to address. Wexley
  maintains that SighSense does not cause breakups. “It only notices when one has
  already begun.”
</p>
`,
    },
  
    {
      id: "dolphin",
      title: "Gift Shop Cashier to Dolphin Translator",
      subtitle: "with Founder of AquaLingo AI",
      episode: "with Coral Finch, Founder & CEO, AquaLingo AI",
      images: ["/images/7.png", "/images/8.png", "/images/9.png"],
      invested: "$21.6M",
      supporters: "1301",
      marketcap: "$2.0M",
      ipo: "D-22",
      audio: "/audio/dolphin.mp3",
      description: `
<p class="svf-page-description">
  Coral Finch previously worked as a cashier in an aquarium gift shop, a role that
  involved ringing up plush dolphins, answering repetitive questions, and
  occasionally refilling the jellyfish keychain display. Over time, Finch began to
  feel that something meaningful was being lost between humans and marine life.
</p>

<p class="svf-page-description">
  “People think dolphins are smiling,” Finch explains. “But that’s projection.”
  This realization eventually led him to found AquaLingo AI, a startup claiming to
  enable direct interspecies communication through artificial intelligence.
</p>

<p class="svf-page-description">
  AquaLingo’s core system allegedly speaks fluent dolphin—but only on Tuesdays
  between 1 and 3 PM. Outside of this window, the model produces no output. Finch
  insists this limitation is not technical, but ethical.
</p>

<p class="svf-page-description">
  Q: Why only Tuesdays?<br />
  Finch: “The model is shy. Dolphins respect that. Constant availability feels
  invasive.”
</p>

<p class="svf-page-description">
  During investor demonstrations, AquaLingo AI plays audio that closely resembles
  someone gargling water into a glass. Finch describes these sounds as “early-stage
  phonemes” and insists they represent emergent linguistic behavior rather than
  noise.
</p>

<p class="svf-page-description">
  The training dataset consists of approximately nine hundred hours of dolphin
  audio recordings, sourced from open marine biology archives, combined with one
  full episode of <em>SpongeBob SquarePants</em>. Finch refers to this combination as
  “balanced exposure.”
</p>

<p class="svf-page-description">
  Despite questions surrounding scientific validity, AquaLingo AI has attracted
  investor interest due to its narrow operating window and refusal to scale
  prematurely. Finch believes this restraint signals maturity. “Most systems talk
  too much,” he says. “Ours listens until it’s comfortable.”
</p>
`,
    },
  
    {
      id: "whisper",
      title: "Librarian to Whisper Decoding Engineer",
      subtitle: "with Founder of WhisperSense AI",
      episode: "with Samuel Drip, Founder & CEO, WhisperSense AI",
      images: ["/images/10.png", "/images/11.png", "/images/12.png"],
      invested: "$11.3M",
      supporters: "538",
      marketcap: "$980K",
      ipo: "D-21",
      audio: "/audio/whisper.mp3",
      description: `
<p class="svf-page-description">
  Samuel Drip spent most of her professional life asking people to be quiet. She
  worked in libraries, archives, and institutional corridors where silence was
  treated less as absence and more as a behavioral standard. Over time, Drip began
  to believe that the softest voices contained the most unfiltered information.
</p>

<p class="svf-page-description">
  “People perform when they speak loudly,” she explains. “Volume introduces
  intention.” This observation became the conceptual basis for WhisperSense AI, a
  startup designed to extract emotional nuance from whispers.
</p>

<p class="svf-page-description">
  The system requires users to whisper exactly three centimeters from the
  microphone. Any deviation—closer or farther—causes the model to disengage.
  According to Drip, this constraint is essential. “Truth has a distance,” she
  says.
</p>

<p class="svf-page-description">
  Q: Why whispers?<br />
  Drip: “Loud speech performs. Whispers confess.”
</p>

<p class="svf-page-description">
  During internal testing, WhisperSense frequently responds with emotionally
  precise yet practically useless feedback, including messages such as “hydration
  recommended,” “uncertainty detected,” and “emotional throat tension present.”
  The company describes this output as “interpretive guidance.”
</p>

<p class="svf-page-description">
  The AI explicitly refuses to analyze confident speakers. Users who project vocal
  certainty receive a system message stating that “confidence obscures signal.”
  WhisperSense is therefore incompatible with motivational speakers and managers.
</p>

<p class="svf-page-description">
  Early investors praised the product’s selective nature and its refusal to engage
  with clarity. Drip maintains that WhisperSense is not designed to help users
  communicate better. “It only notices what slips out when you stop trying,” she
  says.
</p>
`,
    },
  
    {
      id: "sniffcast",
      title: "Dog Trainer to AI Weather Prophet",
      subtitle: "with CEO of SniffCast AI",
      episode: "with Jenna Puffin, Founder & CEO, SniffCast AI",
      images: ["/images/13.png", "/images/14.png", "/images/15.png"],
      invested: "$15.9M",
      supporters: "587",
      marketcap: "$1.3M",
      ipo: "D-17",
      description: `
      <p class="svf-page-description">
        Before founding SniffCast AI, Jonah Puffin spent over a decade training dogs to
        detect cheese, truffles, and what he describes as “emotional ripeness.” During
        this time, he began noticing that dogs often reacted to changes in the weather
        long before any visible signs appeared.
      </p>
      
      <p class="svf-page-description">
        “They’d suddenly stop caring about the cheese,” Puffin explains. “That’s when I
        realized something atmospheric was happening.” This observation eventually
        became the conceptual foundation for SniffCast AI.
      </p>
      
      <p class="svf-page-description">
        The company’s core product attempts to replicate canine intuition using a
        robotic nose roughly the size of a microwave. The device inhales ambient air,
        pauses briefly, and then begins to shake violently before printing a forecast.
        During public demonstrations, predictions have included outputs such as “maybe
        cloudy,” “uncertain drizzle,” and “air feels suspicious.”
      </p>
      
      <p class="svf-page-description">
        Q: How accurate is the system?<br />
        Puffin: “It’s at least as reliable as national weather services on Mondays.”
      </p>
      
      <p class="svf-page-description">
        Unlike traditional forecasting models, SniffCast AI does not analyze
        temperature, pressure, or satellite imagery. Instead, it relies entirely on
        what Puffin calls “olfactory intuition.”
      </p>
      
      <p class="svf-page-description">
        A premium version of SniffCast promises storm detection, humidity awareness,
        and the identification of what the company refers to as “bad vibes.”
      </p>
      
      <p class="svf-page-description">
        Investors described the product as “refreshingly ambiguous,” prompting rapid
        funding despite limited scientific validation.
      </p>
      `, 
     },

      {
        id: "securitycam",
        title: "Watching Empty Hallways as Data",
        subtitle: "with Founder of PatrolNet AI",
        episode: "with Alan Brooks, Founder & CEO, PatrolNet AI",
        images: ["/images/46.png", "/images/47.png", "/images/48.png"],
        invested: "$24.0M",
        supporters: "911",
        marketcap: "$1.4M",
        ipo: "D-18",
        description: `
        <p class="svf-page-description">
          Alan Brooks spent twelve years working night security, walking empty corridors
          and monitoring office buildings long after everyone had gone home. Most shifts
          passed without incident, but Brooks began to notice that the absence of events
          still carried structure.
        </p>
        
        <p class="svf-page-description">
          “Nothing happening is rarely random,” she explains. “It has a rhythm.” During
          long nights in front of static surveillance feeds, Brooks developed an interest
          in what she calls patterned uneventfulness.
        </p>
        
        <p class="svf-page-description">
          This observation eventually led to the founding of PatrolNet AI, a startup that
          analyzes surveillance footage in which no events occur. The system does not
          identify threats, intrusions, or anomalies. Instead, it predicts future moments
          in which nothing is also expected to happen.
        </p>
        
        <p class="svf-page-description">
          Q: What does the AI actually detect?<br />
          Brooks: “Potential uneventfulness.”
        </p>
        
        <p class="svf-page-description">
          PatrolNet’s forecasts include statements such as “low likelihood of surprise,”
          “continued normalcy anticipated,” and “nothing of note expected.” According to
          Brooks, these outputs provide reassurance rather than insight.
        </p>
        
        <p class="svf-page-description">
          The system relies on vast quantities of visually identical footage. Brooks
          rejects concerns about redundancy. “Consistency is the dataset,” she says. “If
          something different happened, we would exclude it.”
        </p>
        
        <p class="svf-page-description">
          Despite openly acknowledging that most footage analyzed by PatrolNet AI is
          indistinguishable, the company raised $24 million in funding. Investors described
          the product as “deeply calming” and praised its ability to confirm stability
          without intervention.
        </p>
        
        <p class="svf-page-description">
          Brooks insists PatrolNet is not designed to prevent incidents. “It doesn’t stop
          anything,” she explains. “It just reassures you that nothing was going to happen
          anyway.”
        </p>
        `,      
      },
    
      {
        id: "flowers",
        title: "When Flowers Become Predictive",
        subtitle: "with Founder of PetalLogic AI",
        episode: "with Elaine Wu, Founder & CEO, PetalLogic AI",
        images: ["/images/49.png", "/images/50.png", "/images/51.png"],
        invested: "$39.1M",
        supporters: "1112",
        marketcap: "$870K",
        ipo: "TBD",
        description: `
<p class="svf-page-description">
  Before entering the technology sector, Elaine Wu worked as a wedding florist,
  arranging bouquets for ceremonies she describes as “optimistic but statistically
  unstable.” Over time, she began to notice that floral arrangements often carried
  emotional tension long before any vows were exchanged.
</p>

<p class="svf-page-description">
  “Flowers absorb decision-making energy,” Wu explains. “They’re present at the
  exact moment people convince themselves.” This belief eventually led her to
  question whether bouquets could function as predictive objects rather than
  decorative ones.
</p>

<p class="svf-page-description">
  PetalLogic AI emerged from this inquiry. The system analyzes petal angles, bouquet
  symmetry, and spatial imbalance to forecast relationship outcomes. According to
  Wu, uneven arrangements often correlate with unresolved negotiations.
</p>

<p class="svf-page-description">
  Q: What data trained the model?<br />
  Wu: “Mostly weddings that didn’t last.”
</p>

<p class="svf-page-description">
  The AI categorizes bouquets using labels such as “optimistic arrangement,”
  “structural denial,” and “temporary alignment.” Wu emphasizes that the system
  does not judge relationships, only their compositional integrity.
</p>

<p class="svf-page-description">
  PetalLogic deliberately ignores flowers that have been rearranged after the
  ceremony. “Once reality intervenes, the signal is contaminated,” Wu says. Dried
  flowers are also excluded for being “emotionally retrospective.”
</p>

<p class="svf-page-description">
  Despite flowers’ short lifespan, PetalLogic AI raised $39.1 million in funding.
  When asked whether this contradiction concerns her, Wu replied, “So do
  startups.” Investors reportedly described the product as “romantically
  analytical.”
</p>
`,

      },
    
      {
        id: "sunglasses",
        title: "Reading Denial Through Sunglasses",
        subtitle: "with Founder of ShadeMind AI",
        episode: "with Trevor Nolan, Founder & CEO, ShadeMind AI",
        images: ["/images/52.png", "/images/53.png", "/images/54.png"],
        invested: "$31.7M",
        supporters: "604",
        marketcap: "$990K",
        ipo: "D-27",
        description: `
<p class="svf-page-description">
  Trevor Nolan spent several years selling sunglasses at a beachside kiosk, where
  customers routinely tried on dozens of frames regardless of weather conditions.
  Over time, Nolan began to suspect that most purchases had little to do with sun
  protection.
</p>

<p class="svf-page-description">
  “People weren’t blocking light,” she explains. “They were avoiding recognition.”
  This observation led Nolan to conclude that sunglasses function less as optical
  tools and more as emotional filters.
</p>

<p class="svf-page-description">
  That insight became the foundation of ShadeMind AI, a startup that analyzes
  sunglass selfies to assess levels of self-awareness. The system examines frame
  size, lens opacity, head tilt, and facial tension to generate what Nolan calls
  “reflective avoidance metrics.”
</p>

<p class="svf-page-description">
  Q: How reliable is it?<br />
  Nolan: “Emotionally accurate.”
</p>

<p class="svf-page-description">
  ShadeMind AI performs poorly when analyzing mirrored lenses, which Nolan refers
  to as “aggressive avoidance.” In these cases, the system often returns incomplete
  results or flags the user as “unavailable for introspection.”
</p>

<p class="svf-page-description">
  The company does not attempt to correct or improve user self-awareness. “We’re
  not here to remove the sunglasses,” Nolan says. “We just acknowledge why they’re
  being worn.”
</p>

<p class="svf-page-description">
  ShadeMind AI raised $31.7 million in funding after Nolan delivered her investor
  pitch while wearing sunglasses indoors. Backers described the presentation as
  “conceptually consistent” and praised the product’s refusal to confront users
  directly.
</p>
`,
      },
    
      {
        id: "trashbins",
        title: "Office Trash as Corporate Insight",
        subtitle: "with Founder of TrashTime AI",
        episode: "with Raul Mendoza, Founder & CEO, TrashTime AI",
        images: ["/images/55.png", "/images/56.png", "/images/57.png"],
        invested: "$58.0M",
        supporters: "2004",
        marketcap: "$3.2M",
        ipo: "D-3",
        audio: "/audio/trash.mp3",
        description: `
<p class="svf-page-description">
  Raul Mendoza spent nearly twenty years cleaning office buildings after hours,
  emptying trash bins long after employees had gone home. Over time, she began to
  notice that discarded objects accumulated in patterns that felt less random
  than emotional.
</p>

<p class="svf-page-description">
  “You can tell how a company feels by what it throws away,” Mendoza explains.
  Half-eaten lunches, printed emails, broken pens, and untouched corporate
  merchandise formed what she describes as “organizational residue.”
</p>

<p class="svf-page-description">
  This observation became the basis for TrashTime AI, a startup that analyzes
  office waste to classify workplace morale. The system categorizes bins into
  emotional states such as “hopeful clutter,” “managed frustration,” and “silent
  resignation.”
</p>

<p class="svf-page-description">
  Q: Does the AI store images?<br />
  Mendoza: “No. The bin has already moved on.”
</p>

<p class="svf-page-description">
  TrashTime AI does not retain visual records of discarded items. According to
  Mendoza, preservation would contradict the philosophy of disposal. “Trash only
  tells the truth once,” she says.
</p>

<p class="svf-page-description">
  The system ignores recycling bins entirely, citing what Mendoza calls “performative
  responsibility.” Only general waste is considered emotionally reliable.
</p>

<p class="svf-page-description">
  Investors described TrashTime AI as ESG-aligned and socially conscious, though
  no one asked how the classifications were generated. Mendoza considers this a
  sign of success. “If they don’t want to look inside,” she notes, “the system is
  working.”
</p>
`,

      },
    
      {
        id: "keyboard",
        title: "Typing Rhythm as Inner Biography",
        subtitle: "with Founder of KeyBeast AI",
        episode: "with Nora Keane, Founder & CEO, KeyBeast AI",
        images: ["/images/58.png", "/images/59.png", "/images/60.png"],
        invested: "$22.0M",
        supporters: "1410",
        marketcap: "$1.2M",
        ipo: "D-26",
        audio: "/audio/keyboard.mp3",
        description: `
<p class="svf-page-description">
  Nora Keane repaired typewriters for years, restoring broken keys and uneven
  springs for clients who preferred mechanical resistance to digital speed.
  During this work, she became increasingly attentive to the rhythm with which
  people typed.
</p>

<p class="svf-page-description">
  “Typing is never neutral,” Keane explains. “It’s a negotiation between intention
  and hesitation.” She began to suspect that typing cadence revealed more about a
  person’s internal state than the words themselves.
</p>

<p class="svf-page-description">
  This belief led to the creation of KeyBeast AI, a system that assigns spirit
  animals based on typing rhythm. Users type a short phrase, often something
  mundane, and the AI responds with classifications such as “confused goat,”
  “mildly disappointed otter,” or “overcommitted pigeon.”
</p>

<p class="svf-page-description">
  Q: What does accuracy mean here?<br />
  Keane: “Recognition.”
</p>

<p class="svf-page-description">
  KeyBeast AI does not attempt to verify whether the assigned animals are correct.
  Instead, it measures how quickly users accept the result. According to Keane,
  immediate agreement is treated as confirmation.
</p>

<p class="svf-page-description">
  The system ignores content entirely, focusing only on timing, pauses, and
  corrective keystrokes. Autocorrect is disabled by default, as Keane considers it
  “emotionally dishonest.”
</p>

<p class="svf-page-description">
  KeyBeast AI went viral after users began sharing their assigned animals online
  without skepticism. Keane views this behavior as validation. “People don’t share
  what they doubt,” she says. “They share what already feels true.”
</p>
`,
      },
    
      {
        id: "chairsound",
        title: "Listening Closely to Chairs",
        subtitle: "with Founder of SeatTone AI",
        episode: "with Lionel Hart, Founder & CEO, SeatTone AI",
        images: ["/images/61.png", "/images/62.png", "/images/63.png"],
        invested: "$14.5M",
        supporters: "688",
        marketcap: "$920K",
        ipo: "D-25",
        description: `
<p class="svf-page-description">
  Lionel Hart believes that chairs understand people better than people understand
  themselves. After years of observing how individuals sit, shift, and hesitate
  before fully committing their weight, Hart became convinced that furniture
  registers emotional conflict.
</p>

<p class="svf-page-description">
  “People lie constantly,” Hart explains. “Mostly to themselves. Chairs notice
  first.” This belief led him to found SeatTone AI, a startup dedicated to analyzing
  the subtle sounds produced when a person sits down.
</p>

<p class="svf-page-description">
  The system records creaks, compressions, and material resistance, translating
  these signals into emotional classifications. A soft creak often yields
  diagnoses such as “internal turbulence” or “unresolved adjustment.” Silence is
  interpreted as avoidance.
</p>

<p class="svf-page-description">
  Q: What about beanbags?<br />
  Hart: “They refuse structure.”
</p>

<p class="svf-page-description">
  SeatTone AI excludes non-rigid seating entirely, arguing that chairs without
  form cannot support introspection. Office stools are treated cautiously, while
  ergonomic chairs are flagged as “emotionally aspirational.”
</p>

<p class="svf-page-description">
  The system does not attempt to resolve discomfort or improve posture. According
  to Hart, intervention would distort the signal. “We don’t correct the sit,” he
  says. “We listen to it.”
</p>

<p class="svf-page-description">
  SeatTone AI raised $14.5 million from investors interested in what Hart calls
  “seated introspection.” Backers praised the product’s ability to surface
  emotional instability without requiring movement, reflection, or follow-up.
</p>
`,
      },
    
      {
        id: "clouds",
        title: "Cloud Shapes and Emotional Forecasts",
        subtitle: "with Founder of SkyFeel AI",
        episode: "with Jasper Wren, Founder & CEO, SkyFeel AI",
        images: ["/images/64.png", "/images/65.png", "/images/66.png"],
        invested: "$18.2M",
        supporters: "903",
        marketcap: "$1.7M",
        ipo: "D-11",
        description: `
<p class="svf-page-description">
  Jasper Wren claims that the sky recognizes your emotional state before you are
  prepared to name it. Long before founding a startup, Wren spent hours lying on
  rooftops and sidewalks, observing cloud movement without attempting to interpret
  it productively.
</p>

<p class="svf-page-description">
  “People look up when they’re already confused,” he explains. “The sky just gets
  there first.” This belief eventually became the conceptual foundation for
  SkyFeel AI, a system designed to map emotional states from photographs of clouds.
</p>

<p class="svf-page-description">
  SkyFeel AI analyzes shape diffusion, contrast drift, and perceived directionality
  within cloud formations. The majority of outputs classify user mood as “drifting
  toward mild disappointment,” a result Wren considers statistically appropriate.
</p>

<p class="svf-page-description">
  Users are encouraged to submit multiple images throughout the day. However,
  repeated submissions often yield identical results. “Consistency is the signal,”
  Wren notes. “If it changes too much, something’s wrong.”
</p>

<p class="svf-page-description">
  Rain presents a significant challenge for the system. During precipitation,
  SkyFeel AI frequently returns a single output: “???” repeated across the screen.
  According to Wren, this is not a failure.
</p>

<p class="svf-page-description">
  “Rain overwhelms the model,” he says. “And that’s honest.” The company has chosen
  not to patch this limitation, arguing that emotional clarity rarely occurs during
  storms.
</p>

<p class="svf-page-description">
  Investors praised SkyFeel AI for its refusal to force interpretation when none
  is available. Wren describes the product not as a diagnostic tool, but as
  “emotional weather reporting without forecasts.”
</p>
`,

      },
    
      {
        id: "drawers",
        title: "Drawer Sounds and Internal Calm",
        subtitle: "with Founder of DrawerSense AI",
        episode: "with Lena Marr, Founder & CEO, DrawerSense AI",
        images: ["/images/67.png", "/images/68.png", "/images/69.png"],
        invested: "$13.0M",
        supporters: "694",
        marketcap: "$1.6M",
        ipo: "D-20",
        audio: "/audio/drawer.mp3",
        description: `
<p class="svf-page-description">
  Lena Marr developed DrawerSense AI after years of quietly observing how people
  interact with storage furniture. He became particularly interested in moments
  when individuals hesitated before opening a drawer, as if negotiating with
  whatever might be inside.
</p>

<p class="svf-page-description">
  “People don’t open drawers casually,” Marr explains. “They prepare.” According
  to him, drawers function as temporary containers for unresolved decisions,
  forgotten intentions, and items meant to remain unseen.
</p>

<p class="svf-page-description">
  DrawerSense AI analyzes opening speed, resistance, and friction patterns to
  determine emotional states. A slow, uneven pull often results in outputs such as
  “emotional clutter detected,” while abrupt movements are labeled “avoidant
  efficiency.”
</p>

<p class="svf-page-description">
  The system ignores drawer contents entirely. Marr insists that what matters is
  not what is stored, but how the drawer is approached. “The object already knows
  it’s hidden,” he says. “The motion tells us why.”
</p>

<p class="svf-page-description">
  Sliding drawers are explicitly excluded from analysis. Marr describes them as
  “overconfident mechanisms” that remove hesitation from the interaction. “They
  don’t pause,” he explains. “They commit too easily.”
</p>

<p class="svf-page-description">
  DrawerSense AI does not offer corrective feedback or organizational advice. Its
  role is strictly observational. According to Marr, intervention would disrupt
  the emotional signal embedded in the motion.
</p>

<p class="svf-page-description">
  Early investors were drawn to the system’s refusal to inspect contents and its
  focus on movement alone. Marr describes DrawerSense as “emotional diagnostics for
  storage behavior,” designed to surface tension without resolving it.
</p>
`,

      },
    
      {
        id: "pillows",
        title: "Pillow Adjustments as Sleep Data",
        subtitle: "with Founder of PillowMind AI",
        episode: "with Clara Wint, Founder & CEO, PillowMind AI",
        images: ["/images/70.png", "/images/71.png", "/images/72.png"],
        invested: "$9.9M",
        supporters: "431",
        marketcap: "$680K",
        ipo: "D-37",
        description: `
<p class="svf-page-description">
  Clara Wint claims that pillows quietly absorb unresolved thoughts during the
  moments just before sleep. After years of paying close attention to how people
  adjusted their pillows at night, he became convinced that restlessness follows
  a discernible pattern.
</p>

<p class="svf-page-description">
  “People think they’re settling in,” Wint explains. “But they’re actually
  negotiating.” According to him, the way a pillow is fluffed, folded, or pushed
  aside reflects decisions that were postponed throughout the day.
</p>

<p class="svf-page-description">
  This belief led to the creation of PillowMind AI, a system that evaluates mental
  restlessness by observing pre-sleep pillow adjustments. The AI measures pressure,
  repetition, and micro-delays between movements to generate emotional summaries.
</p>

<p class="svf-page-description">
  A single fluff often produces the result “resolution unlikely.” Multiple
  adjustments are interpreted as “persistent internal dialogue.” No movement at
  all is flagged as “premature acceptance.”
</p>

<p class="svf-page-description">
  PillowMind AI does not analyze sleep quality, dreams, or duration. Wint insists
  that sleep itself is already compromised by the time it begins. “The pillow
  knows first,” he says.
</p>

<p class="svf-page-description">
  Memory foam pillows are explicitly unsupported. Wint describes them as
  “emotionally rigid,” arguing that materials designed to remember shape interfere
  with honest uncertainty. Feather pillows, by contrast, are considered “highly
  expressive.”
</p>

<p class="svf-page-description">
  Investors were drawn to PillowMind AI’s narrow focus and refusal to offer
  solutions. Wint describes the system not as a sleep aid, but as “a confirmation
  layer for thoughts that refused to settle.”
</p>
`,
      },
    
      {
        id: "micropauses",
        title: "Pauses Before Speech as Insight",
        subtitle: "with Founder of PauseMetric AI",
        episode: "with Marta Liu, Founder & CEO, PauseMetric AI",
        images: ["/images/73.png", "/images/74.png", "/images/75.png"],
        invested: "$16.8M",
        supporters: "845",
        marketcap: "$1.1M",
        ipo: "D-19",
        audio: "/audio/pause.mp3",
        description: `
<p class="svf-page-description">
  Marta Liu developed PauseMetric AI after becoming increasingly uncomfortable
  with how quickly people answered questions. He noticed that immediate responses
  often felt rehearsed, while hesitation carried more informational weight.
</p>

<p class="svf-page-description">
  “Silence happens before intention,” Liu explains. “That’s where the data lives.”
  This belief led him to focus not on speech itself, but on the moments just before
  it begins.
</p>

<p class="svf-page-description">
  PauseMetric AI analyzes micro-pauses that occur prior to verbal response. The
  system classifies these silences into emotional categories such as “reflective,”
  “avoidant,” and “over-negotiated.” According to Liu, the duration of hesitation
  matters more than its cause.
</p>

<p class="svf-page-description">
  Q: Does it analyze words?<br />
  Liu: “Words interfere.”
</p>

<p class="svf-page-description">
  The AI actively ignores spoken content. Users who answer too quickly are labeled
  “temporally evasive,” a designation Liu insists is descriptive rather than
  critical. Longer pauses are treated as evidence of internal processing, even
  when no meaningful answer follows.
</p>

<p class="svf-page-description">
  PauseMetric AI struggles in environments where silence is socially enforced,
  such as elevators or libraries. In these cases, the system frequently returns
  “ambient hesitation,” which Liu describes as “non-actionable but sincere.”
</p>

<p class="svf-page-description">
  Investors expressed interest in the product’s ability to surface discomfort
  without requiring conversation. Liu positions PauseMetric not as a communication
  tool, but as “a delay detector for thoughts that have not yet decided.”
</p>
`,

      },

  {
    id: "walls",
    title: "Staring at Walls as Motivation Data",
    subtitle: "with Founder of WallSense AI",
    episode: "with Patrick Loom, Founder & CEO, WallSense AI",
    images: ["/images/76.png", "/images/77.png", "/images/78.png"],
    invested: "$14.2M",
    supporters: "612",
    marketcap: "$980K",
    ipo: "D-28",
    description: `
<p class="svf-page-description">
  Patrick Loom worked night shifts in a warehouse where visibility was constant and
  stimulation was minimal. For hours at a time, there was nothing to look at
  except blank concrete walls, marked only by scuffs and faded safety lines.
</p>

<p class="svf-page-description">
  “When there’s nothing to look at, you start measuring how long you keep looking,”
  Loom explains. Over time, he became interested in the relationship between
  sustained attention and internal motivation.
</p>

<p class="svf-page-description">
  This observation led to the creation of WallSense AI, a system that evaluates
  motivation by measuring how long users stare at a blank surface. The AI tracks
  duration, blink frequency, and micro head movements to determine motivational
  status.
</p>

<p class="svf-page-description">
  During early testing, a continuous nineteen-second gaze produced the result
  “Motivation: dormant. Directional intent: not loading.” Longer durations often
  returned identical outputs, which Loom considers confirmation rather than error.
</p>

<p class="svf-page-description">
  WallSense explicitly excludes decorated walls, textured surfaces, and any form
  of visual interest. Loom insists that decoration introduces hope, which he
  describes as “a contaminant in motivational analysis.”
</p>

<p class="svf-page-description">
  The system does not attempt to increase productivity, inspire action, or offer
  guidance. According to Loom, motivation cannot be repaired through observation.
</p>

<p class="svf-page-description">
  “WallSense doesn’t fix anything,” Loom says. “It only confirms what’s already
  missing.” Investors reportedly found this restraint reassuring, citing the
  product’s refusal to intervene as a sign of analytical maturity.
</p>
`,
  },

  {
    id: "bags",
    title: "Carrying Tote Bags as Career Signals",
    subtitle: "with Founder of ToteLogic AI",
    episode: "with Simon Kaye, Founder & CEO, ToteLogic AI",
    images: ["/images/79.png", "/images/80.png", "/images/81.png"],
    invested: "$18.7M",
    supporters: "834",
    marketcap: "$1.4M",
    ipo: "D-16",
    description: `
<p class="svf-page-description">
  Simon Kaye began paying attention to tote bags while working in retail spaces
  where customers drifted rather than arrived. Over time, he noticed that people
  carried identical bags with vastly different levels of conviction.
</p>

<p class="svf-page-description">
  “It wasn’t about what was inside,” Kaye explains. “It was how they held it.”
  According to him, grip tension and shoulder angle revealed more about a person’s
  career state than résumés or conversations ever did.
</p>

<p class="svf-page-description">
  This observation led to the creation of ToteLogic AI, a system that evaluates
  career alignment by analyzing how users carry tote bags. The AI measures hand
  pressure, arm swing asymmetry, and posture drift to generate professional
  classifications.
</p>

<p class="svf-page-description">
  A loose grip frequently returns the result “courteous misplacement,” indicating
  polite participation without directional commitment. A tightened grip suggests
  “temporary confidence,” often followed by fatigue.
</p>

<p class="svf-page-description">
  Backpacks are explicitly excluded from analysis. Kaye describes them as “overly
  decisive containers” that imply planning, preparation, and forward intent. “We
  don’t measure commitment,” he says. “We measure ambiguity.”
</p>

<p class="svf-page-description">
  ToteLogic AI does not recommend career changes or next steps. Its function is
  limited to identifying alignment gaps between posture and aspiration.
</p>

<p class="svf-page-description">
  Investors described the product as “non-confrontational career insight” and
  praised its ability to surface professional uncertainty without triggering
  action. Kaye considers this the system’s core strength.
</p>
`,
  },

  {
    id: "waiting",
    title: "Waiting Without Distraction as Burnout Index",
    subtitle: "with Founder of WaitIndex AI",
    episode: "with Nina Cole, Founder & CEO, WaitIndex AI",
    images: ["/images/82.png", "/images/83.png", "/images/84.png"],
    invested: "$22.5M",
    supporters: "1094",
    marketcap: "$2.1M",
    ipo: "D-10",
    description: `
<p class="svf-page-description">
  Cole spent years observing people in transitional spaces—lobbies, platforms,
  hallways—where movement paused without resolution. He became particularly
  interested in moments when individuals waited without distraction, hands idle,
  eyes unfocused.
</p>

<p class="svf-page-description">
  “Waiting reveals exhaustion faster than work,” Cole explains. “Especially when
  there’s nothing to scroll.” He noticed that posture subtly collapsed during
  unoccupied waiting, long before verbal complaints emerged.
</p>

<p class="svf-page-description">
  This observation led to the development of WaitIndex AI, a system that measures
  burnout by analyzing posture, weight shifts, and micro-movements while users
  wait without checking their phones. The AI ignores duration in favor of bodily
  hesitation.
</p>

<p class="svf-page-description">
  During internal testing, eleven seconds of uninterrupted waiting produced the
  result “Burnout: simmering.” Longer waits often returned the same classification,
  which Cole interprets as confirmation rather than escalation.
</p>

<p class="svf-page-description">
  Immediate arrivals cancel the reading entirely. Cole describes this condition as
  “incomplete waiting,” explaining that anticipation without delay lacks emotional
  depth.
</p>

<p class="svf-page-description">
  WaitIndex AI does not operate in environments where waiting is expected to be
  productive, such as airports with lounges or cafés with menus. “Comfort masks
  fatigue,” Cole notes.
</p>

<p class="svf-page-description">
  Investors praised WaitIndex AI for quantifying burnout without requiring surveys
  or self-reporting. Cole positions the system not as a wellness tool, but as
  “a measurement of patience after it has already expired.”
</p>
`,
   },

  {
    id: "notebooks",
    title: "Empty Notebooks as Latent Intention",
    subtitle: "with Creator of NotebookAI",
    episode: "with Theo Marsh, Creator of NotebookAI",
    images: ["/images/85.png", "/images/86.png", "/images/87.png"],
    invested: "$7.8M",
    supporters: "359",
    marketcap: "$520K",
    ipo: "D-63",
    audio: "/audio/notebooks.mp3",
    description: `
<p class="svf-page-description">
  Marsh developed NotebookAI after years of collecting unused notebooks, each
  purchased with a specific intention that was never fully articulated. She
  became increasingly interested in the emotional weight of blank pages.
</p>

<p class="svf-page-description">
  “A notebook is most honest before it’s written in,” Marsh explains. “Once you
  write something down, you’ve already negotiated with reality.” This belief led
  her to question whether potential could be measured prior to action.
</p>

<p class="svf-page-description">
  NotebookAI scans completely blank notebooks to estimate what Marsh calls
  “potential density.” The system analyzes page count, binding tension, paper
  opacity, and the absence of marks to generate predictive summaries.
</p>

<p class="svf-page-description">
  In one early test, a completely empty notebook produced the result “High
  intention, low execution.” According to Marsh, this output represents an optimal
  state of readiness.
</p>

<p class="svf-page-description">
  Partially filled notebooks are excluded from analysis. Marsh describes them as
  “compromised artifacts,” arguing that once a page has been used, possibility
  collapses into documentation.
</p>

<p class="svf-page-description">
  NotebookAI does not read handwriting, content, or language. In fact, the presence
  of writing disables the system entirely. “Ideas behave best before they exist,”
  Marsh says.
</p>

<p class="svf-page-description">
  The product has gained popularity among users who purchase notebooks as a form
  of planning without execution. Marsh views this as validation. “Buying the
  notebook is the plan,” she explains. “Writing would be redundant.”
</p>
`,
  },

  {
    id: "doors",
    title: "Closing Doors as Confidence Evidence",
    subtitle: "with Founder of CloseSense AI",
    episode: "with Jenna Morrow, Founder & CEO, CloseSense AI",
    images: ["/images/88.png", "/images/89.png", "/images/90.png"],
    invested: "$10.9M",
    supporters: "503",
    marketcap: "$860K",
    ipo: "D-34",
    description: `
<p class="svf-page-description">
  Jenna Morrow began paying attention to doors after noticing how differently
  people closed them at the end of conversations. Some doors clicked softly,
  others shut with unnecessary force, and a few were left partially open without
  explanation.
</p>

<p class="svf-page-description">
  “Closing a door is a decision people think is over,” Morrow explains. “But the
  sound keeps negotiating.” This observation led her to develop CloseSense AI, a
  system designed to evaluate confidence through door-closing acoustics.
</p>

<p class="svf-page-description">
  CloseSense AI analyzes volume, resonance, and decay time to generate confidence
  scores. A soft click typically produces an output such as “Confidence: 42%,”
  while a forceful slam triggers classifications like “Overwhelmed” or “Premature
  resolve.”
</p>

<p class="svf-page-description">
  The system does not attempt to contextualize why a door was closed. According to
  Morrow, intention contaminates the signal. “We only listen to what remains,” she
  says.
</p>

<p class="svf-page-description">
  Revolving doors are explicitly unsupported. Morrow describes them as mechanisms
  of “emotional indecision,” noting that their continuous motion prevents any
  definitive conclusion.
</p>

<p class="svf-page-description">
  CloseSense AI does not offer coaching, correction, or improvement strategies.
  Morrow describes the system as honest but not helpful. “It tells you where you
  are,” she explains. “Not where to go.”
</p>

<p class="svf-page-description">
  Investors expressed interest in the product’s ability to quantify confidence
  without engagement. Morrow believes this restraint is intentional. “Confidence
  changes the moment you try to adjust it,” she says.
</p>
`,
  },

  {
    id: "grapes",
    title: "Eating a Single Grape as Life Insight",
    subtitle: "with Founder of GrapeVision AI",
    episode: "with Milan Ortega, Founder & CEO, GrapeVision AI",
    images: ["/images/91.png", "/images/92.png", "/images/93.png"],
    invested: "$19.4M",
    supporters: "947",
    marketcap: "$1.6M",
    ipo: "D-14",
    audio: "/audio/grapes.mp3",
    description: `
<p class="svf-page-description">
  Milan Ortega developed GrapeVision AI after becoming increasingly dissatisfied
  with multi-step self-assessment tools. He began to suspect that most life
  decisions were already encoded in much smaller gestures.
</p>

<p class="svf-page-description">
  “If someone can’t decide how to eat a grape,” Ortega explains, “they already
  know everything they’re avoiding.” This belief led him to focus on a single,
  controlled action rather than extended behavioral analysis.
</p>

<p class="svf-page-description">
  GrapeVision AI determines life direction by observing how a user eats exactly
  one grape. The system analyzes hesitation time, bite pressure, and facial
  micro-adjustments to generate existential classifications.
</p>

<p class="svf-page-description">
  Hesitation before biting frequently produces the output “Unfinished ambition.”
  Immediate consumption is labeled “premature certainty,” while prolonged
  examination results in “direction deferred.”
</p>

<p class="svf-page-description">
  Ortega insists that one grape is sufficient. According to him, introducing
  additional grapes dilutes the signal. “More data is just anxiety pretending to
  be thorough,” he says.
</p>

<p class="svf-page-description">
  Additional grapes are therefore categorized as “overthinking” and excluded from
  analysis. The system resets after each reading to prevent comparative insight.
</p>

<p class="svf-page-description">
  Investors praised GrapeVision AI for its efficiency and refusal to scale beyond
  necessity. Ortega positions the product not as a guidance system, but as “a
  moment of clarity that expires immediately.”
</p>
`,
  },

  {
    id: "corners",
    title: "Hesitating at Corners as Navigation Data",
    subtitle: "with Founder of CornerCast AI",
    episode: "with Jonah Mills, Founder & CEO, CornerCast AI",
    images: ["/images/94.png", "/images/95.png", "/images/96.png"],
    invested: "$16.1M",
    supporters: "721",
    marketcap: "$1.3M",
    ipo: "D-23",
    description: `
<p class="svf-page-description">
  Jonah Mills developed CornerCast AI after years of noticing how people slowed
  down at hallway intersections without fully stopping. He became interested in
  these brief hesitations, which he describes as “decisions that never announce
  themselves.”
</p>

<p class="svf-page-description">
  “You learn more at the corner than on the path,” Mills explains. “Straight lines
  don’t require belief.” This observation led him to focus on spatial moments where
  direction is implied but not yet chosen.
</p>

<p class="svf-page-description">
  CornerCast AI analyzes micro-hesitations that occur when individuals approach
  hallway intersections. The system measures pause duration, foot repositioning,
  and shoulder alignment to evaluate decisiveness.
</p>

<p class="svf-page-description">
  A brief pause frequently returns the classification “Existential drift present.”
  Longer hesitation often produces “Deferred orientation,” while immediate turns
  are labeled “Unexamined certainty.”
</p>

<p class="svf-page-description">
  The system performs reliably only in environments with clearly defined edges.
  Open-plan spaces disable CornerCast entirely, which Mills attributes to the
  absence of boundaries. “Without edges, there’s no decision,” he says.
</p>

<p class="svf-page-description">
  CornerCast AI does not recommend routes or optimize navigation. Its sole function
  is to identify moments where direction collapses into movement without
  resolution.
</p>

<p class="svf-page-description">
  Investors were drawn to the product’s spatial minimalism and its refusal to
  operate in ambiguity-rich layouts. Mills describes CornerCast as “a diagnostic
  for choice that occurs too late to change.”
</p>
`,
  },

  {
    id: "counters",
    title: "Leaning on Counters as Emotional Support",
    subtitle: "with Founder of CounterSense AI",
    episode: "with Brielle Soto, Founder & CEO, CounterSense AI",
    images: ["/images/97.png", "/images/98.png", "/images/99.png"],
    invested: "$12.6M",
    supporters: "588",
    marketcap: "$1.0M",
    ipo: "D-31",
    description: `
<p class="svf-page-description">
  Brielle Soto began paying attention to counters after noticing how often people
  leaned on them without realizing it. Kitchens, cafés, and reception desks became
  informal sites of pause, where bodies rested briefly before continuing on.
</p>

<p class="svf-page-description">
  “People lean when they don’t want to sit,” Soto explains. “It’s support without
  commitment.” She became interested in how much pressure people applied during
  these moments and what that pressure might reveal.
</p>

<p class="svf-page-description">
  This observation led to the development of CounterSense AI, a system that records
  pressure patterns when users lean against counters. The AI measures weight
  distribution, duration, and micro-adjustments to classify emotional reliance.
</p>

<p class="svf-page-description">
  A light lean typically produces the output “Support dependency: low,” while
  sustained pressure is interpreted as “Deferred reliance.” Brief contact followed
  by withdrawal is labeled “Independent hesitation.”
</p>

<p class="svf-page-description">
  Marble counters are explicitly excluded from analysis. Soto describes them as
  “emotionally cold,” noting that their hardness discourages honest leaning. Wood
  and laminate surfaces are considered more expressive.
</p>

<p class="svf-page-description">
  CounterSense AI does not provide recommendations or ergonomic advice. Soto
  describes counters as “silent collaborators,” insisting that their role is to
  receive weight, not resolve it.
</p>

<p class="svf-page-description">
  Investors were drawn to the system’s ability to surface emotional dependency
  without requiring interaction. Soto positions CounterSense not as a support
  system, but as “a record of how often people ask for help without asking.”
</p>
`,
  },

  {
    id: "shoes",
    title: "Removing Shoes as Commitment Signal",
    subtitle: "with Founder of ShoeOff AI",
    episode: "with Lena Brooks, Founder & CEO, ShoeOff AI",
    images: ["/images/100.png", "/images/101.png", "/images/102.png"],
    invested: "$21.3M",
    supporters: "1001",
    marketcap: "$2.4M",
    ipo: "D-7",
    description: `
<p class="svf-page-description">
  The founder of ShoeOff AI became interested in commitment not through contracts
  or long-term planning, but through entrances. Over time, he noticed that people
  revealed more about their intentions while removing their shoes than while
  discussing them.
</p>

<p class="svf-page-description">
  “Shoes are the last thing you take off before staying,” he explains. “How you
  remove them matters.” This observation led to the development of ShoeOff AI, a
  system that evaluates commitment by analyzing shoe-removal behavior.
</p>

<p class="svf-page-description">
  ShoeOff AI records motion sequence, knot tension, and hesitation during the
  removal process. Careful untying frequently produces the classification “Cautious
  detachment,” while rushed removal is labeled “Conditional presence.”
</p>

<p class="svf-page-description">
  The system places particular emphasis on symmetry. Removing one shoe and pausing
  before the second often results in “Unbalanced intent.” Sitting down to untie
  shoes is interpreted as “Prepared endurance.”
</p>

<p class="svf-page-description">
  Slip-on shoes are explicitly excluded from analysis. The founder describes them
  as “emotionally ambiguous exits,” arguing that the absence of laces removes the
  opportunity for reflection. “Without resistance,” he says, “there’s no signal.”
</p>

<p class="svf-page-description">
  ShoeOff AI does not assess destination, duration, or outcome. Its focus remains
  entirely on the threshold moment between arrival and decision.
</p>

<p class="svf-page-description">
  The system is currently used in corporate wellness pilots, where it is presented
  as a non-invasive indicator of engagement. The founder insists ShoeOff AI is not
  about staying or leaving, but about “how carefully people prepare to do either.”
</p>
`,
  },

  {
    id: "stretching",
    title: "Morning Stretching as Purpose Trace",
    subtitle: "with Founder of StretchSignal AI",
    episode: "with Dr. Leslie Wynn, Founder & CEO, StretchSignal AI",
    images: ["/images/103.png", "/images/104.png", "/images/105.png"],
    invested: "$18.0M",
    supporters: "909",
    marketcap: "$1.7M",
    ipo: "D-11",
    audio: "/audio/stretching.mp3",
    description: `
<p class="svf-page-description">
  Leslie Wynn became interested in morning behavior after noticing that the first
  movement of the day often happened before conscious thought. While most people
  described mornings as rushed or unformed, Wynn focused on the single second
  between waking and intention.
</p>

<p class="svf-page-description">
  “The body reacts before the mind negotiates,” he explains. This belief led to the
  development of StretchSignal AI, a system that analyzes a one-second clip of a
  user’s first stretch after waking.
</p>

<p class="svf-page-description">
  StretchSignal AI categorizes stretch direction, extension length, and asymmetry
  to generate emotional summaries. An upward stretch frequently returns the result
  “Longing,” while lateral movements are classified as “Denial.” According to
  Wynn, the direction matters more than the effort.
</p>

<p class="svf-page-description">
  The system does not evaluate flexibility, health, or physical condition. Wynn
  insists these metrics distract from intention. “We’re not measuring capability,”
  he says. “We’re measuring readiness.”
</p>

<p class="svf-page-description">
  Users who do not stretch at all are labeled “data absence enigmas.” StretchSignal
  AI does not attempt to infer meaning from this absence, nor does it prompt users
  to move. Wynn describes non-movement as “a complete but uninterpretable signal.”
</p>

<p class="svf-page-description">
  StretchSignal AI does not provide guidance, routines, or corrective feedback.
  According to Wynn, instruction contaminates clarity. “The moment you tell
  someone how to stretch,” he explains, “you lose what the stretch was trying to
  say.”
</p>

<p class="svf-page-description">
  Investors praised the product’s minimal data requirement and its refusal to
  improve outcomes. Wynn positions StretchSignal not as a wellness tool, but as
  “clarity without instruction.”
</p>
`,

  },
  {
    id: "escalators",
    title: "Standing Still on Escalators",
    subtitle: "with Founder of AutoRise AI",
    episode: "with Victor Hale, Founder & CEO, AutoRise AI",
    images: ["/images/106.png", "/images/107.png", "/images/108.png"],
    invested: "$20.3M",
    supporters: "1022",
    marketcap: "$2.2M",
    ipo: "D-8",
    description: `
<p class="svf-page-description">
  Victor Hale began noticing escalator behavior during his daily commute to a
  consulting job he openly disliked. Each morning, he observed the same vertical
  corridor filled with people making subtly different decisions while moving in
  the same direction.
</p>

<p class="svf-page-description">
  “Everyone is going up,” Hale explains. “But not everyone is progressing.” He
  became fixated on the contrast between those who walked and those who stood
  still, allowing the machine to do the work.
</p>

<p class="svf-page-description">
  This observation led to the founding of AutoRise AI, a system designed to explain
  ambition through escalator behavior. The AI analyzes posture, foot alignment, and
  gaze direction while users stand on escalators to determine motivational posture.
</p>

<p class="svf-page-description">
  Standing motionless is classified as “delegated momentum,” indicating trust in
  systems rather than self-initiation. Walking upward produces the label
  “self-authored progress,” which Hale describes as aspirational but unnecessary.
</p>

<p class="svf-page-description">
  Q: What about running?<br />
  Hale: “Running introduces panic.”
</p>

<p class="svf-page-description">
  AutoRise AI does not operate on broken escalators. Hale cites “emotional
  instability in vertical systems,” noting that stalled movement introduces false
  urgency and compromises interpretive clarity.
</p>

<p class="svf-page-description">
  The system does not account for destination, urgency, or physical ability. Its
  sole concern is how willingly individuals surrender upward motion to machinery.
</p>

<p class="svf-page-description">
  Investors praised AutoRise AI for making ambition feel measurable without
  requiring effort. Hale positions the product not as a motivator, but as “a mirror
  for progress people were already outsourcing.”
</p>
`,

  },
  {
    id: "screenshots",
    title: "Saving Screenshots Never Revisited",
    subtitle: "with Founder of ScreenVault AI",
    episode: "with Noah Kim, Founder & CEO, ScreenVault AI",
    images: ["/images/109.png", "/images/110.png", "/images/111.png"],
    invested: "$27.1M",
    supporters: "1508",
    marketcap: "$3.4M",
    ipo: "D-4",
    audio: "/audio/screenshots.mp3",
    description: `
<p class="svf-page-description">
  Noah Kim founded ScreenVault AI after realizing she had accumulated thousands of
  screenshots she never opened. Notifications, messages, maps, confirmations—all
  captured, stored, and then quietly ignored. Kim began to wonder whether the act
  of saving had replaced the act of responding.
</p>

<p class="svf-page-description">
  “Screenshots feel like responsibility postponed,” Kim explains. “You save them
  so you don’t have to decide yet.” Over time, she became interested less in what
  the images contained and more in why they were never revisited.
</p>

<p class="svf-page-description">
  ScreenVault AI was built to analyze abandoned screenshots in order to identify
  what Kim calls “deferred relevance.” The system tracks how long files remain
  untouched, how deeply they are buried in folders, and whether they are ever
  renamed.
</p>

<p class="svf-page-description">
  Q: What kind of screenshots matter most?<br />
  Kim: “Messages you were afraid to answer.”
</p>

<p class="svf-page-description">
  The AI does not read image content. According to Kim, interpretation would
  introduce accountability. Instead, ScreenVault measures avoidance through time,
  treating delay itself as the primary signal.
</p>

<p class="svf-page-description">
  Screenshots deleted immediately are labeled “impulsive clarity.” Files that
  persist for months without interaction are categorized as “pending emotional
  load.” Screenshots that resurface only during storage cleanups are marked
  “retroactive guilt.”
</p>

<p class="svf-page-description">
  Kim describes ScreenVault not as a memory tool, but as “memory without
  accountability.” Enterprise clients have expressed interest in applying the
  model to internal documentation hoarding, where files are saved, duplicated, and
  never acted upon.
</p>

<p class="svf-page-description">
  Investors praised the product’s refusal to interpret content and its focus on
  duration alone. Kim believes this restraint is essential. “If we knew what the
  screenshots meant,” she says, “we might have to do something about them.”
</p>
`,

  },
  {
    id: "emails",
    title: "Drafting Emails Never Sent",
    subtitle: "with Founder of Unsend AI",
    episode: "with Camille Roth, Founder & CEO, Unsend AI",
    images: ["/images/112.png", "/images/113.png", "/images/114.png"],
    invested: "$34.7M",
    supporters: "1644",
    marketcap: "$3.9M",
    ipo: "D-2",
    description: `
<p class="svf-page-description">
  Camille Roth built Unsend AI after spending years composing emails that never
  left the draft folder. Messages were written, rewritten, softened, clarified,
  and then quietly abandoned. Roth began to suspect that modern communication
  occurred more often in preparation than in delivery.
</p>

<p class="svf-page-description">
  “Drafts are where honesty lingers,” Roth explains. “Once you send something,
  it becomes social.” This observation led him to focus on what he calls
  pre-communicative behavior—the emotional labor that happens before a message is
  released.
</p>

<p class="svf-page-description">
  Unsend AI analyzes unsent email drafts to classify emotional hesitation. The
  system measures length, revision frequency, deletion patterns, and time spent
  idle in the compose window to generate interpretive labels.
</p>

<p class="svf-page-description">
  Long drafts are frequently categorized as “conflict rehearsal,” indicating
  unresolved confrontation practiced in private. Short drafts that undergo
  repeated edits are labeled “polite distress,” a state Roth describes as “careful
  discomfort.”
</p>

<p class="svf-page-description">
  Q: What happens if the email is finally sent?<br />
  Roth: “The data collapses.”
</p>

<p class="svf-page-description">
  Once a draft is sent, Unsend AI immediately invalidates the session. According
  to Roth, delivery contaminates the signal. “Resolution erases hesitation,” he
  says. “And hesitation is the point.”
</p>

<p class="svf-page-description">
  Unsend AI markets itself as an insight tool for organizations that value
  reflection over resolution. The platform integrates with internal email systems
  but intentionally avoids tracking responses or outcomes.
</p>

<p class="svf-page-description">
  Critics argue that the product rewards avoidance and discourages clarity. Roth
  rejects this interpretation. “Avoidance is communication,” he insists. “It’s
  just communication that hasn’t decided who it’s for yet.”
</p>
`,
  },
  {
    id: "laundry",
    title: "Leaving Laundry in the Machine",
    subtitle: "with Founder of SpinCycle AI",
    episode: "with Marta Feldman, Founder & CEO, SpinCycle AI",
    images: ["/images/115.png", "/images/116.png", "/images/117.png"],
    invested: "$18.9M",
    supporters: "899",
    marketcap: "$1.9M",
    ipo: "D-14",
    audio: "/audio/laundry.mp3",
    description: `
<p class="svf-page-description">
  Marta Feldman founded SpinCycle AI after repeatedly forgetting her laundry in
  shared washing machines. What began as minor inconvenience gradually became a
  pattern—cycles completed, notifications ignored, damp clothes sitting quietly
  behind closed lids.
</p>

<p class="svf-page-description">
  “Laundry finishes long before responsibility does,” Feldman explains. She
  became interested in the emotional gap between completion and retrieval, a space
  where obligation exists but action hesitates.
</p>

<p class="svf-page-description">
  SpinCycle AI tracks the delay between wash completion and user retrieval. Delays
  under ten minutes are classified as “functional responsibility,” indicating
  attentiveness without urgency. Delays exceeding thirty minutes result in the
  label “deferred obligation.”
</p>

<p class="svf-page-description">
  The system does not analyze fabric type, load size, or washing method. Feldman
  insists these variables obscure the central signal. “The machine has already
  done its job,” she says. “What matters is whether the person does.”
</p>

<p class="svf-page-description">
  Q: What about rewashing?<br />
  Feldman: “That resets guilt, not memory.”
</p>

<p class="svf-page-description">
  SpinCycle AI treats rewashing as emotional avoidance rather than correction. The
  system flags repeated cycles as “postponed accountability,” noting that clean
  outcomes do not imply resolved intention.
</p>

<p class="svf-page-description">
  Marketed as a productivity and habit-awareness tool, SpinCycle AI openly admits
  it cannot prevent mildew or missed pickups. Its purpose, Feldman explains, is
  not intervention but recognition.
</p>

<p class="svf-page-description">
  Investors praised the product’s honesty about its limitations. Feldman describes
  SpinCycle as “a mirror for chores that finish themselves while we don’t.”
</p>
`,
  },
  {
    id: "tabs",
    title: "Keeping Browser Tabs Open Overnight",
    subtitle: "with Founder of TabMind AI",
    episode: "with Julian Park, Founder & CEO, TabMind AI",
    images: ["/images/118.png", "/images/119.png", "/images/120.png"],
    invested: "$37.2M",
    supporters: "2011",
    marketcap: "$4.1M",
    ipo: "D-1",
    description: `
<p class="svf-page-description">
  Julian Park began paying attention to browser tabs after noticing how rarely she
  closed them. Articles were opened with intent, messages half-read, documents
  prepared for later that never arrived. Over time, the browser became less a tool
  and more a visible archive of unfinished thinking.
</p>

<p class="svf-page-description">
  “Tabs stay open because decisions don’t,” Park explains. She became interested
  not in what the tabs contained, but in why they were allowed to remain present
  indefinitely.
</p>

<p class="svf-page-description">
  TabMind AI analyzes open browser tabs left overnight, counting quantity,
  persistence, and recurrence. The system assigns cognitive load scores based on
  how long tabs remain open without interaction.
</p>

<p class="svf-page-description">
  More than twenty tabs result in the classification “Deferred processing,” a
  state Park describes as “thinking that refuses to schedule itself.” Tabs that
  refresh automatically are flagged as “persistent anxiety.”
</p>

<p class="svf-page-description">
  Closing all tabs triggers the label “False resolution.” According to TabMind,
  sudden emptiness often indicates symbolic closure rather than genuine clarity.
</p>

<p class="svf-page-description">
  Q: What about tab groups?<br />
  Park: “They’re denial with structure.”
</p>

<p class="svf-page-description">
  TabMind AI does not reduce tabs, recommend extensions, or encourage digital
  hygiene. Its purpose is reframing rather than intervention. “We don’t clean the
  browser,” Park says. “We explain why it’s messy.”
</p>

<p class="svf-page-description">
  Investors praised the system for validating habits users already intended to
  keep. Park positions TabMind not as a productivity tool, but as “permission to
  leave things open without pretending they’re organized.”
</p>
`,

  },
  {
    id: "windows",
    title: "Looking Out Windows Without Purpose",
    subtitle: "with Founder of PaneVision AI",
    episode: "with Sofia Lang, Founder & CEO, PaneVision AI",
    images: ["/images/121.png", "/images/122.png", "/images/123.png"],
    invested: "$26.9M",
    supporters: "1322",
    marketcap: "$2.8M",
    ipo: "D-6",
    audio: "/audio/windows.mp3",
    description: `
<p class="svf-page-description">
  Evelyn Lang developed PaneVision AI after realizing how often she found herself
  standing in front of windows without a clear reason. The gaze was not directed
  outward with curiosity, nor inward with reflection—it simply lingered.
</p>

<p class="svf-page-description">
  “Looking out the window feels like doing something,” Lang explains. “But nothing
  actually happens.” She became interested in these moments of passive orientation,
  where attention is suspended between intention and avoidance.
</p>

<p class="svf-page-description">
  PaneVision AI analyzes unmotivated window gazing to determine what it calls
  future orientation. The system measures gaze duration, head angle, and frequency
  of return to the same window throughout the day.
</p>

<p class="svf-page-description">
  Short glances are interpreted as “situational awareness,” suggesting brief
  environmental checking without emotional investment. Extended gazing produces
  the label “Deferred ambition,” a state Lang describes as “wanting movement
  without selecting a direction.”
</p>

<p class="svf-page-description">
  Curtains are explicitly excluded from analysis. Lang insists that curtains
  corrupt the data by introducing choice. “Once you decide whether to open or
  close them,” she says, “you’ve already acted.”
</p>

<p class="svf-page-description">
  PaneVision AI does not analyze what users are looking at outside the window.
  Buildings, trees, weather, and people are considered distractions. The system
  is concerned only with the act of looking itself.
</p>

<p class="svf-page-description">
  The platform offers no recommendations, alerts, or behavioral nudges. According
  to Lang, suggestion would interrupt the condition being measured. PaneVision
  simply reports what users already suspected while staring outside.
</p>

<p class="svf-page-description">
  Investors praised the product’s restraint and its refusal to promise progress.
  Lang describes PaneVision not as a planning tool, but as “a confirmation that
  waiting can feel like direction.”
</p>
`,

  },
  {
    id: "receipts",
    title: "Keeping Receipts Without Reason",
    subtitle: "with Founder of ReceiptMind AI",
    episode: "with Dana Schultz, Founder & CEO, ReceiptMind AI",
    images: ["/images/124.png", "/images/125.png", "/images/126.png"],
    invested: "$14.8M",
    supporters: "677",
    marketcap: "$1.1M",
    ipo: "D-29",
    description: `
<p class="svf-page-description">
  Daniel Schultz developed ReceiptMind AI after noticing how rarely people returned
  items they clearly intended to. Receipts were folded carefully, placed into
  wallets, and then carried for days without action. The possibility of return
  lingered, but resolution never arrived.
</p>

<p class="svf-page-description">
  “Keeping a receipt feels like responsibility,” Schultz explains. “Returning an
  item feels like admitting a mistake.” He became interested in the emotional
  distance between these two states.
</p>

<p class="svf-page-description">
  ReceiptMind AI examines emotional attachment to proof of purchase. The system
  tracks how receipts are folded, how long they remain stored, and how often they
  are retrieved without being used.
</p>

<p class="svf-page-description">
  A receipt kept longer than five days is classified as “Deferred justification,”
  indicating sustained intention without follow-through. Receipts unfolded and
  refolded multiple times are labeled “Active reconsideration.”
</p>

<p class="svf-page-description">
  Receipts discarded immediately are interpreted as “Instant acceptance,” while
  those photographed and archived digitally are marked “Symbolic accountability.”
</p>

<p class="svf-page-description">
  The system does not analyze purchase price, item category, or retailer. Schultz
  insists these details distract from the emotional signal. “Regret doesn’t care
  what you bought,” he says. “It cares that you might have to explain it.”
</p>

<p class="svf-page-description">
  ReceiptMind AI does not prompt returns or provide reminders. Its purpose is not
  correction, but acknowledgment. Schultz claims the system helps users recognize
  regret without requiring confrontation.
</p>

<p class="svf-page-description">
  Investors praised the product’s ability to surface hesitation without reducing
  spending. Schultz positions ReceiptMind not as a consumer tool, but as “a ledger
  of decisions people prefer to keep folded.”
</p>
`,

  },
  {
    id: "postits",
    title: "Writing Post-it Notes Never Moved",
    subtitle: "with Founder of StickyMind AI",
    episode: "with Harold Lin, Founder & CEO, StickyMind AI",
    images: ["/images/127.png", "/images/128.png", "/images/129.png"],
    invested: "$29.3M",
    supporters: "1440",
    marketcap: "$3.1M",
    ipo: "D-5",
    description: `
<p class="svf-page-description">
  Daniel Lin developed StickyMind AI after noticing that certain Post-it notes
  seemed to outlive their original purpose. Tasks were written, placed carefully,
  and then quietly absorbed into the background of daily life.
</p>

<p class="svf-page-description">
  “Post-it notes don’t disappear when the task is forgotten,” Lin explains. “They
  just become furniture.” He became interested in the moment when reminders stop
  functioning as prompts and start functioning as symbols.
</p>

<p class="svf-page-description">
  StickyMind AI analyzes Post-it notes that remain in the same location for extended
  periods of time. The system tracks adhesive decay, edge curling, paper
  discoloration, and how often the note is visually passed without being touched.
</p>

<p class="svf-page-description">
  Notes that are never relocated are classified as “Symbolic intention,”
  indicating commitment that has been acknowledged but indefinitely postponed.
  Notes that slowly slide downward are labeled “Gradual disengagement.”
</p>

<p class="svf-page-description">
  Q: What if the task is completed?<br />
  Lin: “Then the note should be removed.”
</p>

<p class="svf-page-description">
  StickyMind AI treats removed notes as resolved signals and immediately ends
  analysis. According to Lin, completion leaves no residue worth measuring.
</p>

<p class="svf-page-description">
  The system does not analyze handwriting, task difficulty, or urgency. Lin insists
  these details distract from what matters most. “The problem isn’t what the note
  says,” he explains. “It’s that it’s still there.”
</p>

<p class="svf-page-description">
  Marketed as a productivity tool, StickyMind AI quietly documents unresolved
  commitments without offering solutions. Lin positions the platform not as a
  reminder system, but as “a record of promises that have settled into the room.”
</p>
`,
  },
  {
    id: "fridge",
    title: "Opening the Fridge Repeatedly",
    subtitle: "with Founder of ColdSearch AI",
    episode: "with Marcus Vale, Founder & CEO, ColdSearch AI",
    images: ["/images/130.png", "/images/131.png", "/images/132.png"],
    invested: "$21.9M",
    supporters: "1108",
    marketcap: "$2.3M",
    ipo: "D-9",
    audio: "/audio/fridge.mp3",
    description: `
<p class="svf-page-description">
  Oliver Vale conceived ColdSearch AI after noticing how often he opened the
  refrigerator without intending to eat anything. The door would open, cold air
  would escape, and nothing would change. Minutes later, the same motion would
  repeat.
</p>

<p class="svf-page-description">
  “People don’t open the fridge for food,” Vale explains. “They open it for the
  possibility of resolution.” He became interested in the gap between expectation
  and consumption, where desire circulates without direction.
</p>

<p class="svf-page-description">
  ColdSearch AI analyzes repeated refrigerator openings without consumption. Each
  opening is logged as an “Expectation refresh,” a momentary check to see whether
  circumstances have improved since the last glance.
</p>

<p class="svf-page-description">
  Multiple openings within five minutes result in the classification “Nutritional
  uncertainty.” According to Vale, this state reflects a search for clarity rather
  than appetite. “Hunger is rarely about food,” he insists.
</p>

<p class="svf-page-description">
  The system distinguishes between closing behaviors. Slowly closing the fridge
  door is interpreted as “Residual hope,” suggesting belief that something inside
  might eventually satisfy. Slamming the door returns “Frustration,” indicating a
  breakdown in expectation.
</p>

<p class="svf-page-description">
  ColdSearch AI does not analyze food type, expiration dates, or caloric value.
  Vale describes these variables as distractions. “If the answer were nutritional,
  the door wouldn’t open again.”
</p>

<p class="svf-page-description">
  Marketed as a behavioral insight tool, ColdSearch does not discourage repeated
  openings or suggest meals. Its function is purely observational, documenting
  moments when people search for change in a place that remains the same.
</p>

<p class="svf-page-description">
  Investors praised the system’s ability to quantify dissatisfaction without
  intervention. Vale positions ColdSearch not as a diet tool, but as “a record of
  hope cooling in plain sight.”
</p>
`,

  },
  {
    id: "lights",
    title: "Turning Lights On During Daytime",
    subtitle: "with Founder of DayGlow AI",
    episode: "with Renata Silva, Founder & CEO, DayGlow AI",
    images: ["/images/133.png", "/images/134.png", "/images/135.png"],
    invested: "$12.4M",
    supporters: "569",
    marketcap: "$940K",
    ipo: "D-36",
    description: `
<p class="svf-page-description">
  Marco Silva began noticing lighting behavior after realizing how often he turned
  on lamps during broad daylight. Rooms were already bright, windows uncovered,
  yet switches were still flipped with quiet certainty.
</p>

<p class="svf-page-description">
  “Light isn’t always about visibility,” Silva explains. “Sometimes it’s about
  atmosphere catching up with intention.” He became interested in why people
  introduce artificial light when natural light is already sufficient.
</p>

<p class="svf-page-description">
  DayGlow AI tracks instances of artificial lighting during daylight hours. The
  system logs time of activation, ambient brightness, and duration of overlap
  between natural and artificial illumination.
</p>

<p class="svf-page-description">
  Turning lights on at noon is categorized as “Atmospheric control,” a state Silva
  describes as “the desire to author a mood rather than accept one.” Early-morning
  lighting is labeled “Premature readiness,” while evening daylight overlap is
  classified as “Residual alertness.”
</p>

<p class="svf-page-description">
  Q: Is it inefficient?<br />
  Silva: “Efficiency is emotional.”
</p>

<p class="svf-page-description">
  DayGlow AI does not evaluate energy consumption or recommend conservation. Silva
  insists these concerns interfere with interpretation. “If people cared about
  efficiency,” he says, “they wouldn’t turn the lights on.”
</p>

<p class="svf-page-description">
  The system explicitly excludes smart lighting and automated schedules. According
  to Silva, these technologies introduce excessive intentionality and contaminate
  the signal. “Automation hides indecision,” he explains.
</p>

<p class="svf-page-description">
  Marketed as an environmental insight tool, DayGlow AI offers no optimization or
  corrective feedback. It simply documents moments when users choose control over
  daylight.
</p>

<p class="svf-page-description">
  Investors praised the platform for reframing inefficiency as expression. Silva
  positions DayGlow not as an energy product, but as “a record of moods turned on
  manually.”
</p>
`,
  },
  {
    id: "sockets",
    title: "Plugging Chargers Without Devices",
    subtitle: "with Founder of IdleCharge AI",
    episode: "with Mina Cho, Founder & CEO, IdleCharge AI",
    images: ["/images/136.png", "/images/137.png", "/images/138.png"],
    invested: "$14.1M",
    supporters: "631",
    marketcap: "$1.1M",
    ipo: "D-28",
    description: `
<p class="svf-page-description">
  Hannah Cho developed IdleCharge AI after noticing how often charging cables
  remained plugged into outlets without any device attached. The cables rested on
  desks, floors, and nightstands, quietly connected to power but serving no
  immediate purpose.
</p>

<p class="svf-page-description">
  “People don’t unplug cables because they’re waiting,” Cho explains. “Not for a
  device—but for a moment when they’ll need one.” She became interested in unused
  power as a form of expectation rather than waste.
</p>

<p class="svf-page-description">
  IdleCharge AI monitors charging cables that remain connected to outlets without
  devices attached. Each instance is logged as “Preparatory optimism,” a state Cho
  describes as readiness without commitment.
</p>

<p class="svf-page-description">
  The system tracks duration, cable position, and proximity to frequently used
  surfaces. Cables left plugged in overnight are flagged as “Sustained readiness,”
  while those unplugged and reconnected repeatedly are labeled “Anxious
  anticipation.”
</p>

<p class="svf-page-description">
  Cho insists that unused power is not inefficiency, but belief. “If people didn’t
  expect future need,” she says, “they would disconnect entirely.”
</p>

<p class="svf-page-description">
  Power strips are explicitly excluded from analysis. According to Cho, they
  represent collective intent and dilute individual signal. “Shared outlets hide
  personal expectation,” she explains.
</p>

<p class="svf-page-description">
  IdleCharge AI does not measure energy loss or suggest unplugging. Its purpose is
  not conservation, but acknowledgment. The system documents moments when power is
  made available before necessity arrives.
</p>

<p class="svf-page-description">
  Investors praised the platform for reframing waste as optimism. Cho positions
  IdleCharge not as an energy tool, but as “a record of belief left quietly
  charging.”
</p>
`,

  },
  {
    id: "mirrorshops",
    title: "Trying on Clothes Without Buying",
    subtitle: "with Founder of FitExit AI",
    episode: "with Lena Hoffman, Founder & CEO, FitExit AI",
    images: ["/images/139.png", "/images/140.png", "/images/141.png"],
    invested: "$33.6M",
    supporters: "1702",
    marketcap: "$3.8M",
    ipo: "D-2",
    audio: "/audio/fittingroom.mp3",
    description: `
<p class="svf-page-description">
  Lila Hoffman developed FitExit AI after realizing how much time she spent in
  fitting rooms without buying anything. Clothes were tried on, adjusted, and
  examined from multiple angles before being returned to hangers exactly as they
  were found.
</p>

<p class="svf-page-description">
  “Fitting rooms feel decisive,” Hoffman explains. “But most decisions end before
  they reach the cashier.” She became interested in the emotional labor that occurs
  between entering a fitting room and leaving it unchanged.
</p>

<p class="svf-page-description">
  FitExit AI evaluates self-alignment by tracking time spent inside fitting rooms
  without purchases. The system measures mirror interaction duration, posture
  shifts, garment handling frequency, and hesitation before re-dressing.
</p>

<p class="svf-page-description">
  Extended mirror engagement produces the classification “Identity sampling,” a
  state Hoffman describes as “trying on futures that never leave the room.”
  Frequent glances away from the mirror are flagged as “Discomfort negotiation.”
</p>

<p class="svf-page-description">
  Immediate exit without mirror interaction returns “Resolution without
  acquisition,” indicating acceptance without exploration. According to Hoffman,
  this reflects clarity rather than decisiveness.
</p>

<p class="svf-page-description">
  The system does not evaluate fit, size accuracy, price, or trend relevance.
  Hoffman insists these variables interfere with interpretation. “If the issue
  were clothing,” she says, “the mirror wouldn’t matter this much.”
</p>

<p class="svf-page-description">
  Hoffman describes fitting rooms as “temporary futures,” spaces where alternate
  versions of the self appear briefly and dissolve without consequence. FitExit AI
  does not recommend purchases or styling changes.
</p>

<p class="svf-page-description">
  Marketed as a behavioral insight tool, FitExit quietly documents moments when
  identity is tested but not adopted. Investors praised the system for capturing
  self-alignment without requiring ownership.
</p>
`,
  },

  {
    id: "alarm",
    title: "Dismissing Alarms Without Snoozing",
    subtitle: "with Founder of WakeIntent AI",
    episode: "with Colin Mercer, Founder & CEO, WakeIntent AI",
    images: ["/images/142.png", "/images/143.png", "/images/144.png"],
    invested: "$24.8M",
    supporters: "1189",
    marketcap: "$2.6M",
    ipo: "D-7",
    description: `
<p class="svf-page-description">
  Daniel Mercer began paying attention to alarms after realizing that waking up was
  rarely a single decision. Each morning involved a sequence of gestures—dismiss,
  snooze, silence—performed before any conscious plan had fully formed.
</p>

<p class="svf-page-description">
  “Alarms aren’t about sleep,” Mercer explains. “They’re about intention being
  interrupted.” He became interested in how people negotiate with time before
  agreeing to enter the day.
</p>

<p class="svf-page-description">
  WakeIntent AI analyzes how users dismiss alarms rather than when they wake up.
  The system tracks response speed, repetition, and interval patterns to infer
  motivational posture.
</p>

<p class="svf-page-description">
  Immediate dismissal without snoozing is labeled “Premature resolve,” indicating
  decisiveness formed before readiness. According to Mercer, this often masks
  urgency rather than clarity.
</p>

<p class="svf-page-description">
  Repeated snoozing produces the classification “Negotiated awakening,” a state in
  which intention is postponed through incremental agreement. Extended snooze
  chains are marked “Deferred consent.”
</p>

<p class="svf-page-description">
  WakeIntent AI does not analyze sleep quality, duration, or circadian rhythm.
  Mercer insists these metrics obscure what matters most. “Sleep explains how you
  feel,” he says. “Alarms explain what you do about it.”
</p>

<p class="svf-page-description">
  The system does not recommend wake times or reduce snoozing. Its purpose is not
  optimization, but recognition. WakeIntent documents the moment intention first
  encounters resistance.
</p>

<p class="svf-page-description">
  Investors praised the platform for reframing alarms as behavioral signals rather
  than productivity failures. Mercer positions WakeIntent not as a sleep tool, but
  as “a record of how the day is reluctantly accepted.”
</p>
`,
  },
];
  
  export default cards;
  